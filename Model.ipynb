{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images in F:\\university\\milan\\second semester\\deep life\\main proj\\CNN_Cell_Segmentation\\Data\\train\\01\n",
      "torch.Size([8, 1, 696, 520]) torch.Size([8, 1, 696, 520])\n"
     ]
    }
   ],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))]\n",
    "        self.transform = transform\n",
    "\n",
    "        # Debug print statements\n",
    "        print(f\"Found {len(self.image_filenames)} images in {image_dir}\")\n",
    "        if len(self.image_filenames) == 0:\n",
    "            raise ValueError(f\"No images found in {image_dir}. Please check the directory path and file extensions.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_name).convert('L')  # Convert to grayscale\n",
    "\n",
    "        mask_name = os.path.join(self.mask_dir, self.image_filenames[idx])\n",
    "        mask = Image.open(mask_name).convert('L')  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Transform for normalizing and resizing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((696, 520)),  # Resize images to 696x520\n",
    "    transforms.ToTensor(),  # Convert PIL image to Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# Paths to your image and mask folders\n",
    "image_dir = r'F:\\university\\milan\\second semester\\deep life\\main proj\\CNN_Cell_Segmentation\\Data\\train\\01'\n",
    "mask_dir = r'F:\\university\\milan\\second semester\\deep life\\main proj\\CNN_Cell_Segmentation\\Data\\train\\01_MASKS'\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "dataset = CustomImageDataset(image_dir=image_dir, mask_dir=mask_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Debug print to verify DataLoader\n",
    "for images, masks in dataloader:\n",
    "    print(images.shape, masks.shape)\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double convolution block with padding\n",
    "def double_conv(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.down_conv_1 = double_conv(1, 64)\n",
    "        self.down_conv_2 = double_conv(64, 128)\n",
    "        self.down_conv_3 = double_conv(128, 256)\n",
    "        self.down_conv_4 = double_conv(256, 512)\n",
    "        self.down_conv_5 = double_conv(512, 1024)\n",
    "\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.up_conv_1 = double_conv(1024, 512)\n",
    "\n",
    "        self.up_trans_2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up_conv_2 = double_conv(512, 256)\n",
    "\n",
    "        self.up_trans_3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up_conv_3 = double_conv(256, 128)\n",
    "        \n",
    "        self.up_trans_4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv_4 = double_conv(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, 2, kernel_size=1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Encoder\n",
    "        x1 = self.down_conv_1(image)\n",
    "        x2 = self.max_pool_2x2(x1)\n",
    "        x3 = self.down_conv_2(x2)\n",
    "        x4 = self.max_pool_2x2(x3)\n",
    "        x5 = self.down_conv_3(x4)\n",
    "        x6 = self.max_pool_2x2(x5)\n",
    "        x7 = self.down_conv_4(x6)\n",
    "        x8 = self.max_pool_2x2(x7)\n",
    "        x9 = self.down_conv_5(x8)\n",
    "\n",
    "        # Decoder\n",
    "        #Before each concatenation in the decoder, we check if the sizes of the tensors match. If they don't, we use nn.functional.pad to pad the smaller tensor to match the dimensions of the larger tensor.This ensures that the tensors have the same dimensions before concatenation without cropping.\n",
    "        x = self.up_trans_1(x9)\n",
    "        if x.size() != x7.size():\n",
    "            diffY = x7.size()[2] - x.size()[2]\n",
    "            diffX = x7.size()[3] - x.size()[3]\n",
    "            x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = self.up_conv_1(torch.cat([x, x7], 1))\n",
    "\n",
    "        x = self.up_trans_2(x)\n",
    "        if x.size() != x5.size():\n",
    "            diffY = x5.size()[2] - x.size()[2]\n",
    "            diffX = x5.size()[3] - x.size()[3]\n",
    "            x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = self.up_conv_2(torch.cat([x, x5], 1))\n",
    "\n",
    "        x = self.up_trans_3(x)\n",
    "        if x.size() != x3.size():\n",
    "            diffY = x3.size()[2] - x.size()[2]\n",
    "            diffX = x3.size()[3] - x.size()[3]\n",
    "            x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = self.up_conv_3(torch.cat([x, x3], 1))\n",
    "\n",
    "        x = self.up_trans_4(x)\n",
    "        if x.size() != x1.size():\n",
    "            diffY = x1.size()[2] - x.size()[2]\n",
    "            diffX = x1.size()[3] - x.size()[3]\n",
    "            x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = self.up_conv_4(torch.cat([x, x1], 1))\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 1, 696, 520]), Output shape: torch.Size([8, 2, 696, 520])\n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "\n",
    "# Check the model with one batch of data\n",
    "# Should be [batch_size, 2, 696, 520]\n",
    "for images, masks in dataloader:\n",
    "    outputs = model(images)\n",
    "    print(f\"Input shape: {images.shape}, Output shape: {outputs.shape}\")\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 696, 520])\n"
     ]
    }
   ],
   "source": [
    "#image = torch.rand(1, 1, 696, 520)  # Batch size, channel, height, width\n",
    "#model = UNet()\n",
    "#output = model(image)\n",
    "#print(output.size())  # Should be [1, 2, 696, 520]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
