{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ch_in, ch_out, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=1):\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch, ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64, ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128, ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256, ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512, ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = self.pad_and_concat(x4, d5)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = self.pad_and_concat(x3, d4)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = self.pad_and_concat(x2, d3)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = self.pad_and_concat(x1, d2)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1\n",
    "\n",
    "    def pad_and_concat(self, x1, x2):\n",
    "        diffY = x1.size()[2] - x2.size()[2]\n",
    "        diffX = x1.size()[3] - x2.size()[3]\n",
    "\n",
    "        x2 = nn.functional.pad(x2, (diffX // 2, diffX - diffX // 2,\n",
    "                                    diffY // 2, diffY - diffY // 2))\n",
    "        return torch.cat([x1, x2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84 images in Data/train/01\n",
      "Found 84 masks in Data/train/01_MASKS\n",
      "Found 84 images in Data/test/02\n",
      "Found 84 masks in Data/test/02_MASKS\n"
     ]
    }
   ],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_names = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))]\n",
    "        self.mask_names = [f for f in os.listdir(mask_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))]\n",
    "        self.transform = transform\n",
    "\n",
    "        # Debug print statements\n",
    "        print(f\"Found {len(self.image_names)} images in {image_dir}\")\n",
    "        print(f\"Found {len(self.mask_names)} masks in {mask_dir}\")\n",
    "        \n",
    "        if len(self.image_names) == 0:\n",
    "            raise ValueError(f\"No images found in {image_dir}. Please check the directory path and file extensions.\")\n",
    "        if len(self.mask_names) == 0:\n",
    "            raise ValueError(f\"No masks found in {mask_dir}. Please check the directory path and file extensions.\")\n",
    "        if len(self.image_names) != len(self.mask_names):\n",
    "            print(f\"Number of images: {len(self.image_names)}\")\n",
    "            print(f\"Number of masks: {len(self.mask_names)}\")  \n",
    "            raise ValueError(\"Number of images and masks do not match.\")\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_names[idx])\n",
    "        image = Image.open(img_name).convert('L')  # Convert to grayscale\n",
    "         # Debug print statement\n",
    "        # print(f\"Accessing index: {idx}\")\n",
    "\n",
    "        mask_name = os.path.join(self.mask_dir, self.mask_names[idx])\n",
    "        mask = Image.open(mask_name).convert('L')  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def min_max_normalize(tensor):\n",
    "        min_val = tensor.min()\n",
    "        max_val = tensor.max()\n",
    "        tensor = (tensor - min_val) / (max_val - min_val)\n",
    "        return tensor\n",
    "\n",
    "# Transform for resizing and Min-Max normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((696, 520)),  # Resize images to 696x520\n",
    "    transforms.ToTensor(),  # Convert PIL image to Tensor\n",
    "    min_max_normalize,  # Apply Min-Max normalization\n",
    "])\n",
    "\n",
    "# Paths to your image and mask folders\n",
    "image_dir = 'Data/train/01'\n",
    "mask_dir = 'Data/train/01_MASKS'\n",
    "\n",
    "# Datasets and DataloadersD\n",
    "train_dataset = CustomImageDataset(image_dir=image_dir, mask_dir=mask_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Paths to your image and mask folders for validation(using test)\n",
    "val_image_dir = 'Data/test/02'\n",
    "val_mask_dir = 'Data/test/02_MASKS'\n",
    "\n",
    "# Datasets and Dataloaders for test\n",
    "val_dataset = CustomImageDataset(image_dir=val_image_dir, mask_dir=val_mask_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(SR,GT,threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "    corr = torch.sum(SR==GT)\n",
    "    tensor_size = SR.size(0)*SR.size(1)*SR.size(2)*SR.size(3)\n",
    "    acc = float(corr)/float(tensor_size)\n",
    "\n",
    "    return acc\n",
    "\n",
    "def get_sensitivity(SR,GT,threshold=0.5):\n",
    "    # Sensitivity == Recall\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "    # TP : True Positive\n",
    "    # FN : False Negative\n",
    "    TP = ((SR==1)+(GT==1))==2\n",
    "    FN = ((SR==0)+(GT==1))==2\n",
    "\n",
    "    SE = float(torch.sum(TP))/(float(torch.sum(TP+FN)) + 1e-6)     \n",
    "    \n",
    "    return SE\n",
    "\n",
    "def get_specificity(SR,GT,threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "    # TN : True Negative\n",
    "    # FP : False Positive\n",
    "    TN = ((SR==0)+(GT==0))==2\n",
    "    FP = ((SR==1)+(GT==0))==2\n",
    "\n",
    "    SP = float(torch.sum(TN))/(float(torch.sum(TN+FP)) + 1e-6)\n",
    "    \n",
    "    return SP\n",
    "\n",
    "def get_precision(SR,GT,threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "    # TP : True Positive\n",
    "    # FP : False Positive\n",
    "    TP = ((SR==1)+(GT==1))==2\n",
    "    FP = ((SR==1)+(GT==0))==2\n",
    "\n",
    "    PC = float(torch.sum(TP))/(float(torch.sum(TP+FP)) + 1e-6)\n",
    "\n",
    "    return PC\n",
    "\n",
    "def get_F1(SR,GT,threshold=0.5):\n",
    "    # Sensitivity == Recall\n",
    "    SE = get_sensitivity(SR,GT,threshold=threshold)\n",
    "    PC = get_precision(SR,GT,threshold=threshold)\n",
    "\n",
    "    F1 = 2*SE*PC/(SE+PC + 1e-6)\n",
    "\n",
    "    return F1\n",
    "\n",
    "def get_JS(SR,GT,threshold=0.5):\n",
    "    # JS : Jaccard similarity\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "    \n",
    "    Inter = torch.sum((SR+GT)==2)\n",
    "    Union = torch.sum((SR+GT)>=1)\n",
    "    \n",
    "    JS = float(Inter)/(float(Union) + 1e-6)\n",
    "    \n",
    "    return JS\n",
    "\n",
    "def get_DC(SR,GT,threshold=0.5):\n",
    "    # DC : Dice Coefficient\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "    Inter = torch.sum((SR+GT)==2)\n",
    "    DC = float(2*Inter)/(float(torch.sum(SR)+torch.sum(GT)) + 1e-6)\n",
    "\n",
    "    return DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch 1\n",
      "Working on batch 2\n",
      "Working on batch 3\n",
      "Working on batch 4\n",
      "Working on batch 5\n",
      "Working on batch 6\n",
      "Working on batch 7\n",
      "Working on batch 8\n",
      "Working on batch 9\n",
      "Working on batch 10\n",
      "Working on batch 11\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# images = images.to(device)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# masks = masks.to(device)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m---> 51\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[0;32m     52\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Visualize some images and their predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ARASH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ARASH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ARASH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32mc:\\Users\\ARASH\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "model = U_Net(img_ch=1,output_ch=1)\n",
    "lr=1e-4\n",
    "optimizer = optim.Adam(unet.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCELoss();\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for i, (images, masks) in enumerate(train_loader):\n",
    "        print(f\"Working on batch {i+1}\")\n",
    "        # Move tensors to the configured device (e.g., GPU if available)\n",
    "        # images = images.to(device)\n",
    "        # masks = masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Apply sigmoid to outputs for BCE loss\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)  \n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            # images = images.to(device)\n",
    "            # masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Visualize some images and their predictions\n",
    "            for j in range(min(5, images.size(0))):  # Show up to 5 examples\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                ax[0].imshow(images[j].cpu().numpy().squeeze(), cmap='gray')\n",
    "                ax[0].set_title('Input Image')\n",
    "                ax[1].imshow(masks[j].cpu().numpy().squeeze(), cmap='gray')\n",
    "                ax[1].set_title('True Mask')\n",
    "                ax[2].imshow(outputs[j].cpu().numpy().squeeze(), cmap='gray')\n",
    "                ax[2].set_title('Predicted Mask')\n",
    "                plt.show()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot losses\n",
    "plot_losses(train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
